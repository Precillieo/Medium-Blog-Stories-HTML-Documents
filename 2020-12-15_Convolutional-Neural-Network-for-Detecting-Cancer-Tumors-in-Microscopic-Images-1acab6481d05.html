<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Convolutional Neural Network for Detecting Cancer Tumors in Microscopic Images</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Convolutional Neural Network for Detecting Cancer Tumors in Microscopic Images</h1>
</header>
<section data-field="subtitle" class="p-summary">
How we won first place at the Idealab.Ai Hackathon hosted by the Tanzania government
</section>
<section data-field="body" class="e-content">
<section name="ec29" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="37b7" id="37b7" class="graf graf--h3 graf--leading graf--title">Convolutional Neural Network for Detecting Cancer Tumors in Microscopic Images</h3><h4 name="1f0d" id="1f0d" class="graf graf--h4 graf-after--h3 graf--subtitle">How we won first place at the IdeaLabAi Hackathon hosted by the Sahara Ventures and the Tanzania AI Lab</h4><figure name="f1d0" id="f1d0" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*_HwoUJjix5UhL1etHEICGg.png" data-width="685" data-height="276" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*_HwoUJjix5UhL1etHEICGg.png"><figcaption class="imageCaption">image by SpringerLink</figcaption></figure><p name="d909" id="d909" class="graf graf--p graf-after--figure">Cancer of all types is increasing exponentially in the countries and regions at large. Cervical cancer, which is caused by a certain strain of the Human Papillomavirus (HPV), presents a significant public health threat to women on the African continent. All but one of the top 20 countries worldwide with the highest burden of Cervical cancer in 2018 were in Africa in which Tanzania was among.</p><p name="541f" id="541f" class="graf graf--p graf-after--p">In Tanzania, cancer control activities and services are undertaken by a wide range of government and non-government agencies, most of which have been done by ORCI and to a lesser extent by some other NGOs. If you need detailed information on this, read more about it <a href="https://drive.google.com/file/d/1YLc-22NzJ2to12Qv6mCiKcCTtirH-6nK/view" data-href="https://drive.google.com/file/d/1YLc-22NzJ2to12Qv6mCiKcCTtirH-6nK/view" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>.</p><p name="3dee" id="3dee" class="graf graf--p graf-after--p">AI Commons is a nonprofit organization supported by the ecosystem of AI practitioners, entrepreneurs, academia, NGOs, AI industry players, and organizations/individuals focused on the common good. The organization has gathered the best minds in AI to support the creation of a knowledge hub in AI that can be accessible by anyone, that can help inform governance, policymaking, and investments around the deployment of AI solutions, and be a catalyst for supporting diversity and inclusivity in how AI is deployed for sustainable development goals.</p><p name="3057" id="3057" class="graf graf--p graf-after--p">The AI Commons Health &amp; Wellbeing Hackathon is an online competition to solve identified local health problems in Tanzania utilizing AI in which each team consisted of 5 participants.</p><h4 name="c629" id="c629" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">How It Started</strong></h4><p name="a6fd" id="a6fd" class="graf graf--p graf-after--h4">As a participant who was in search of team members connected with <strong class="markup--strong markup--p-strong">Alfaxad Eyembe</strong>, a Backend Developer and AI Enthusiast, after which notions and ideas were brought down after we brainstormed.</p><p name="1a36" id="1a36" class="graf graf--p graf-after--p">We concluded on building a model comprising of Deep Convolutional Neural Networks(CNN) and a Web App that screens microscopic images so as to detect cancer tumors, thus increasing the speed, accuracy in cancer diagnosis and testing.</p><p name="9bfa" id="9bfa" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Anthony Mipawa</strong>, a Software Engineer, <strong class="markup--strong markup--p-strong">Sang’udi E Sang’udi</strong>, a UI/UX Designer, <strong class="markup--strong markup--p-strong">Salome Rumold Mosha</strong>, the Pitcher, all of which are Tanzanians, were the other members of the team.</p><p name="9c48" id="9c48" class="graf graf--p graf-after--p">I was the only Nigerian, also the only person to take up the Machine Learning Engineer role since our solution wrapped around building a CNN model for image classification.</p><h4 name="eec8" id="eec8" class="graf graf--h4 graf-after--p">Solution Implementation</h4><ul class="postList"><li name="002a" id="002a" class="graf graf--li graf-after--h4">Introduction</li><li name="8aab" id="8aab" class="graf graf--li graf-after--li">Dataset</li><li name="c74e" id="c74e" class="graf graf--li graf-after--li">Data Preprocessing</li><li name="9abc" id="9abc" class="graf graf--li graf-after--li">Methodology</li><li name="0f21" id="0f21" class="graf graf--li graf-after--li">Evaluation</li><li name="f515" id="f515" class="graf graf--li graf-after--li">Deployment</li></ul><h4 name="f8ce" id="f8ce" class="graf graf--h4 graf-after--li"><strong class="markup--strong markup--h4-strong">Introduction</strong></h4><p name="5c73" id="5c73" class="graf graf--p graf-after--h4">Full digitalization of the microscopic evaluation of stained tissue sections in histopathology has become feasible recently due to the advances in slide scanning technology and reduction in digital storage cost. It also increased the computer-aided diagnostic which was why we chose microscopic images to x-ray images as x-ray images will always appear as shades of grey.</p><p name="4773" id="4773" class="graf graf--p graf-after--p">Left to do was to get the tons of microscopic images needed for training and making predictions by classifying future images if they contain cancer tumors or not. However, while researching on different health data platforms, I came across the <a href="https://github.com/basveeling/pcam" data-href="https://github.com/basveeling/pcam" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">PatchCamelyon(PCam)</strong></a> benchmark dataset.</p><h4 name="0fc2" id="0fc2" class="graf graf--h4 graf-after--p">Dataset</h4><p name="88a8" id="88a8" class="graf graf--p graf-after--h4">The PCam is a new and challenging image classification dataset that consists of 327.680 color images (96px, 96px) extracted from histopathologic scans of lymph node sections with each image annotated with a binary label indicating the presence of metastatic tissue.</p><p name="4fda" id="4fda" class="graf graf--p graf-after--p">However, the original PCam dataset contains duplicate images due to its probabilistic sampling. Then luckily, I saw a competition hosted on <a href="https://www.kaggle.com/c/histopathologic-cancer-detection/notebooks" data-href="https://www.kaggle.com/c/histopathologic-cancer-detection/notebooks" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Kaggle</strong></a> to challenge researchers to create an algorithm that identifies metastatic cancer in small image patches taken from larger digital pathology scans, which is a slightly modified version of the PCam dataset.</p><p name="8f54" id="8f54" class="graf graf--p graf-after--p">The Kaggle modified one consists of 220,000 training images and 57,458 test images while all images are 96×96 pixels in size. The challenge was to predict the labels(<strong class="markup--strong markup--p-strong">Tumor</strong>: Positive, <strong class="markup--strong markup--p-strong">No Tumor</strong>: Negative) for the test. A positive label indicates that the center 32x32px region of a patch contains at least one pixel of tumor tissue.</p><p name="2be6" id="2be6" class="graf graf--p graf-after--p">Tumor tissue in the outer region of the patch does not influence the label. This outer region is provided to enable fully-convolutional models that do not use zero-padding, to ensure consistent behavior when applied to a whole-slide image. With pleasure, that was the exact dataset we needed for our model.</p><h4 name="d03e" id="d03e" class="graf graf--h4 graf-after--p">Data Preparation</h4><figure name="5de8" id="5de8" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*tzXFHUjAijwNnBf4Bmm6LA.jpeg" data-width="565" data-height="333" src="https://cdn-images-1.medium.com/max/800/1*tzXFHUjAijwNnBf4Bmm6LA.jpeg"><figcaption class="imageCaption">data labels</figcaption></figure><p name="d68c" id="d68c" class="graf graf--p graf-after--figure">From the class distribution above, we can see that our target values(0 &amp;1) are slightly balanced with class 0(<strong class="markup--strong markup--p-strong">No Tumor</strong>) having about 54.16% and class 1(<strong class="markup--strong markup--p-strong">Tumor</strong>) with about 45.83% of the whole dataset.</p><p name="f452" id="f452" class="graf graf--p graf-after--p">The data preparation raps around using the <strong class="markup--strong markup--p-strong">ImageDataGenerator</strong> class from <strong class="markup--strong markup--p-strong">tf.keras.preprocessing.image, </strong>a host of<strong class="markup--strong markup--p-strong"> Data Augmentation </strong>which encompasses a wide range of techniques used to generate “<strong class="markup--strong markup--p-strong">new</strong>” training samples from the original ones by applying random jitters and perturbations. Different data augmentation techniques on each image before feeding it to the neural network can be seen below.</p><figure name="497e" id="497e" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*yS3LhJUPnm_s3gIyqeAY0w.jpeg" data-width="511" data-height="658" src="https://cdn-images-1.medium.com/max/800/1*yS3LhJUPnm_s3gIyqeAY0w.jpeg"><figcaption class="imageCaption">image data generator</figcaption></figure><ul class="postList"><li name="d911" id="d911" class="graf graf--li graf-after--figure"><strong class="markup--strong markup--li-strong">Normalization</strong> is a parameter in ImageDataGenerator known as <strong class="markup--strong markup--li-strong">rescale. </strong>For most image data, the pixel values are integers with values between 0 and 255. Neural networks process inputs using small weight values, meanwhile, inputs with large integer values can disrupt or slow down the learning process. However, it is good practice to normalize the pixel values so that each pixel value has a value between 0 and 1. So, all we did was to divide all pixel values by 255 which is the largest pixel value.</li><li name="2a75" id="2a75" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Horizontal and Vertical flip, Rotation, Shear, Brightness, Zoom range </strong>Applying a tidbit of transformation to an input image will change its appearance slightly, but it does not change the class label.</li><li name="c96f" id="c96f" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Validation Split </strong>can separate a portion of your training data into a validation dataset and evaluate the performance of your model on that validation dataset. You can do this by setting the validation_split parameter, most times between 0.2 to 0.5 of your training data. Also, the default value of <strong class="markup--strong markup--li-strong">Batch Size </strong>was used(32).</li></ul><p name="b4f8" id="b4f8" class="graf graf--p graf-after--li">Therefore, data preprocessing/augmentation is a very natural, easy method to apply for computer vision tasks.</p><h4 name="2f7c" id="2f7c" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Methodology &amp; Evaluation</strong></h4><p name="a32f" id="a32f" class="graf graf--p graf-after--h4">These are the specific procedures or techniques used to identify, select, process, our proposed solution ranging from architectures, batch normalizations, optimization methods, evaluation methods, and all.</p><p name="1acd" id="1acd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Model Architecture</strong></p><p name="ce57" id="ce57" class="graf graf--p graf-after--p">A Convolutional Neural Network(CNN or ConvNet) is a class of deep neural<strong class="markup--strong markup--p-strong"> </strong>networks, most commonly applied to analyzing visual imagery. CNN was inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. This neural network has one or more layers and it’s mainly for image processing, classification, segmentation, and also for other autocorrelated data. Here goes how the model was compiled:</p><blockquote name="b0c7" id="b0c7" class="graf graf--blockquote graf-after--p">[Conv2D*3 -&gt; MaxPool2D -&gt; Dropout] x4→ (filters = 16, 32, 64, 96)</blockquote><figure name="0b91" id="0b91" class="graf graf--figure graf-after--blockquote"><img class="graf-image" data-image-id="1*uv1QCU0W45VV8iXZaZVEjg.jpeg" data-width="635" data-height="645" src="https://cdn-images-1.medium.com/max/800/1*uv1QCU0W45VV8iXZaZVEjg.jpeg"><figcaption class="imageCaption">model architecture</figcaption></figure><p name="bd26" id="bd26" class="graf graf--p graf-after--figure">Now for the meat of the problem. Four(4) CNN layers were used. We’ll start with a 2D convolution of the image. It’s set up to take <strong class="markup--strong markup--p-strong">16 windows</strong> at first, or “filters”, of each image. CNN is essentially sliding a filter over the input, each filter(kernel size) being 3x3 in size.</p><p name="4b79" id="4b79" class="graf graf--p graf-after--p">The most commonly used A<strong class="markup--strong markup--p-strong">ctivation</strong> F<strong class="markup--strong markup--p-strong">unction</strong> in neural networks, <strong class="markup--strong markup--p-strong">Rectified Linear Units(ReLU) </strong>is used.<strong class="markup--strong markup--p-strong"> </strong>ReLU for short is a piecewise linear function that will output the input directly if it is positive, otherwise, it will output zero.</p><p name="50b9" id="50b9" class="graf graf--p graf-after--p">Moreso, In CNN, the <strong class="markup--strong markup--p-strong">input</strong> layer itself is not a layer, but a tensor. It’s the starting tensor you send to the first hidden layer. This tensor must have the same <strong class="markup--strong markup--p-strong">shape</strong> as your training data. For example, we have images of <strong class="markup--strong markup--p-strong">96x96 </strong>pixels in RGB (3 channels), so, this brought about the shape of our input data being <strong class="markup--strong markup--p-strong">(96, 96, 3).</strong></p><p name="c907" id="c907" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Batch</strong> <strong class="markup--strong markup--p-strong">normalization,</strong> which increases the stability of a neural network, normalizes the output of a previous activation layer by subtracting the <strong class="markup--strong markup--p-strong">batch</strong> mean and dividing by the <strong class="markup--strong markup--p-strong">batch</strong> standard deviation is added. We then run a second convolution on top of that with 16, 3x3 windows. A layer that takes the maximum of each <strong class="markup--strong markup--p-strong">2x2</strong> result to distill the results down into something more manageable is called <strong class="markup--strong markup--p-strong">MaxPooling2D</strong>. A <strong class="markup--strong markup--p-strong">Dropout</strong> filter is then applied to prevent overfitting.</p><p name="cd87" id="cd87" class="graf graf--p graf-after--p">Not all of these are strictly necessary, you could run without pooling and dropout, but those extra steps help avoid overfitting and help things run faster. These steps are repeated in each of the four layers with the windows being changed within this range <strong class="markup--strong markup--p-strong">16&gt;32&gt;64&gt;96.</strong></p><p name="5ab4" id="5ab4" class="graf graf--p graf-after--p">Next, we flattened the 2D layer we have at this stage into a 1D layer. So at this point, we can just pretend we have a traditional multi-layer perception. We then feed that into a hidden, flat layer of 256 units. We then applied dropout again to further prevent overfitting.</p><p name="4fc2" id="4fc2" class="graf graf--p graf-after--p">And finally, we fed that into our final 1 unit where the <strong class="markup--strong markup--p-strong">softmax</strong> <strong class="markup--strong markup--p-strong">function</strong> is applied to choose our binary since it exists between 0 and 1. Therefore, it is especially used for models where we have to predict the probability as an output.</p><h4 name="e201" id="e201" class="graf graf--h4 graf-after--p">Evaluation</h4><figure name="f2e2" id="f2e2" class="graf graf--figure graf-after--h4"><img class="graf-image" data-image-id="1*qsh3QNIFHVz4mZMme8OAww.jpeg" data-width="456" data-height="78" src="https://cdn-images-1.medium.com/max/800/1*qsh3QNIFHVz4mZMme8OAww.jpeg"><figcaption class="imageCaption">optimization and evaluation</figcaption></figure><p name="d7cf" id="d7cf" class="graf graf--p graf-after--figure">The performance of our proposed classification model was evaluated based on accuracy (ACC). Given the number of True Positives (TP), False Positives (FP), True Negatives (TN), and False Negatives (FN), it’s expressed mathematically as seen below:</p><figure name="f176" id="f176" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*jaagAnPto6J0Ls9Kjfw8fQ.png" data-width="1000" data-height="279" src="https://cdn-images-1.medium.com/max/800/1*jaagAnPto6J0Ls9Kjfw8fQ.png"><figcaption class="imageCaption">formula of accuracy</figcaption></figure><p name="ae1d" id="ae1d" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Loss Function</strong></p><p name="489e" id="489e" class="graf graf--p graf-after--p">Since we have two almost perfectly balanced classes and the accuracy to evaluate the model, we’ve used <strong class="markup--strong markup--p-strong">binary cross-entropy</strong> loss function to train our neural networks. The equation for binary cross-entropy loss is:</p><figure name="5685" id="5685" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*PDtIfRHpMfbbXbhj26I-OA.png" data-width="548" data-height="79" src="https://cdn-images-1.medium.com/max/800/1*PDtIfRHpMfbbXbhj26I-OA.png"><figcaption class="imageCaption">picture from stackexchange.com from TowardsDataScience</figcaption></figure><p name="99a5" id="99a5" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Optimizer</strong></p><p name="fe7c" id="fe7c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Adam optimizer</strong>, similar to vanilla stochastic gradient descent in that it&#39;s a first-order, gradient-based algorithm used to optimize stochastic objective functions. It is a cross-breed of the popular <strong class="markup--strong markup--p-strong">RMSProp</strong> and <strong class="markup--strong markup--p-strong">AdaGrad</strong> <strong class="markup--strong markup--p-strong">optimizers</strong> while possessing the attractive properties of both as it works well with sparse gradients and does not require a stationary objective function. It naturally reduces the step size as training proceeds. The algorithm works by keeping track of not only the weight vector but also biased estimates of the first and second moments and the time step. Its pseudo-code can be seen below:</p><figure name="b2c9" id="b2c9" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*QpqKxnzqLExFe2BQittVmA.png" data-width="547" data-height="347" src="https://cdn-images-1.medium.com/max/800/1*QpqKxnzqLExFe2BQittVmA.png"><figcaption class="imageCaption">adam optimizer pseudo code</figcaption></figure><p name="261b" id="261b" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Model Fitting and Training</strong></p><figure name="1de7" id="1de7" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*2zOCIiXHe9ZxygIvsw0BPQ.jpeg" data-width="720" data-height="183" src="https://cdn-images-1.medium.com/max/800/1*2zOCIiXHe9ZxygIvsw0BPQ.jpeg"><figcaption class="imageCaption">fitting parameters</figcaption></figure><p name="fc9d" id="fc9d" class="graf graf--p graf-after--figure">While working towards finding the best parameters for the model, different distinct experiments with various setups were carried out, best setup yielding the top result was selected. The table below shows the list of parameters and their respective values;</p><figure name="45e8" id="45e8" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*BZogn8rHYzjjNsxVSGOKTg.jpeg" data-width="441" data-height="379" src="https://cdn-images-1.medium.com/max/800/1*BZogn8rHYzjjNsxVSGOKTg.jpeg"><figcaption class="imageCaption">parameters and values</figcaption></figure><p name="d5b5" id="d5b5" class="graf graf--p graf-after--figure">Below is a plot showing the training, validation losses, and accuracies of our trained model.</p><figure name="1f91" id="1f91" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*5h5l3H7SyKR6z5mgBFbhyg.jpeg" data-width="520" data-height="613" src="https://cdn-images-1.medium.com/max/800/1*5h5l3H7SyKR6z5mgBFbhyg.jpeg"><figcaption class="imageCaption">accuracies and losses</figcaption></figure><p name="b716" id="b716" class="graf graf--p graf-after--figure">Ultimately, the model was thereafter, saved as a <strong class="markup--strong markup--p-strong">.h5 model </strong>for easy deployment into the existing production environment.<strong class="markup--strong markup--p-strong"> H5 </strong>is a file format to store structured data, it’s not a model by itself. Keras saves models in this format as it can easily store the weights and model configuration in a single file.</p><figure name="039c" id="039c" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*npf1Q6aaorPa9DxPu2-yyw.jpeg" data-width="399" data-height="169" src="https://cdn-images-1.medium.com/max/800/1*npf1Q6aaorPa9DxPu2-yyw.jpeg"><figcaption class="imageCaption">H5 Model</figcaption></figure><h4 name="c4bc" id="c4bc" class="graf graf--h4 graf-after--figure">Deployment</h4><p name="63ce" id="63ce" class="graf graf--p graf-after--h4">As <strong class="markup--strong markup--p-strong">Django</strong> is written in Python, it makes it a great choice of web frameworks for deploying Machine Learning models. <strong class="markup--strong markup--p-strong">Anthony Mipawa</strong> and <strong class="markup--strong markup--p-strong">Alfaxad</strong> <strong class="markup--strong markup--p-strong">Eyembe</strong>, the software guys among us decided to use Django instead of the Flask framework I’m well versed in. The model was deployed in a web app while <strong class="markup--strong markup--p-strong">Sang’udi E Sang’udi</strong> helped in designing the user interface. Pitching and prototype documentation was done by <strong class="markup--strong markup--p-strong">Salome Rumold Mosha </strong>who has past experience in such projects and competitions.</p><h4 name="4f82" id="4f82" class="graf graf--h4 graf-after--p">Conclusion</h4><p name="9ec0" id="9ec0" class="graf graf--p graf-after--h4">We are so proud to provide a novel solution that helps packing the clinically-relevant task of tumor detection into a straight-forward binary image classification task. With an accuracy of 87% on a held-out test set, we are demonstrating the feasibility of this solution. However, our approach can still be improved by using a pre-trained model, ensembling two or three models that tend to outperform single classifiers.</p><p name="9858" id="9858" class="graf graf--p graf-after--p">The sole aim of our prototype is to increase the speed, reliability, and accuracy of cancer detection in Tanzania due to the problem of low doctors to patients ratio. Also, exorbitant death of Tanzanian women without even getting diagnosed as a result of Cervical and Breast cancer.</p><p name="507c" id="507c" class="graf graf--p graf-after--p">Just today, the winners were announced. And guess what? <strong class="markup--strong markup--p-strong">ELIXIR Won!!</strong></p><figure name="ff19" id="ff19" class="graf graf--figure graf--iframe graf-after--p"><iframe src="https://giphy.com/embed/gKOWq5dhdOznBUfpC2/twitter/iframe" width="435" height="244" frameborder="0" scrolling="no"></iframe></figure><p name="ff2f" id="ff2f" class="graf graf--p graf-after--figure">Oh! I forgot to tell you what Elixir is. We named our web app <strong class="markup--strong markup--p-strong">Elixir.</strong></p><p name="b049" id="b049" class="graf graf--p graf-after--p">Here are the links you’d like to see;</p><ul class="postList"><li name="d64e" id="d64e" class="graf graf--li graf-after--p">Code on <a href="https://github.com/Precillieo/Elixir-Cancer-Diagnosis-AI-Based-System" data-href="https://github.com/Precillieo/Elixir-Cancer-Diagnosis-AI-Based-System" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">GitHub</strong></a></li><li name="7a07" id="7a07" class="graf graf--li graf-after--li">Elixir Cancer Diagnosis AI-Based System on <a href="https://youtu.be/Pfgfh4DJToo" data-href="https://youtu.be/Pfgfh4DJToo" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--li-strong">YouTub</strong></a><strong class="markup--strong markup--li-strong">e</strong></li></ul><p name="d73b" id="d73b" class="graf graf--p graf-after--li">Click on the clap icon below, share with friends, coding buddies, mentors, and as many people who may need this. More so, you can connect with me on <a href="https://twitter.com/precillieo" data-href="https://twitter.com/precillieo" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener" target="_blank">Twitter</a> and <a href="https://www.linkedin.com/in/kolawole-precious-8723b2198/" data-href="https://www.linkedin.com/in/kolawole-precious-8723b2198/" class="markup--anchor markup--p-anchor" rel="noopener nofollow noopener noopener" target="_blank">LinkedIn</a> in case of questions and more ideas to share. Happy Reading!</p><h4 name="c5e6" id="c5e6" class="graf graf--h4 graf-after--p">References</h4><div name="b205" id="b205" class="graf graf--mixtapeEmbed graf-after--h4"><a href="https://github.com/basveeling/pcam" data-href="https://github.com/basveeling/pcam" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/basveeling/pcam"><strong class="markup--strong markup--mixtapeEmbed-strong">basveeling/pcam</strong><br><em class="markup--em markup--mixtapeEmbed-em">That which is measured, improves. - Karl Pearson The PatchCamelyon benchmark is a new and challenging image…</em>github.com</a><a href="https://github.com/basveeling/pcam" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="f40d718fda214243bd25c6eac0896c4f" data-thumbnail-img-id="0*_B1Ux7ARTMOrIm6M" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*_B1Ux7ARTMOrIm6M);"></a></div><div name="4224" id="4224" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://www.kaggle.com/c/histopathologic-cancer-detection/overview" data-href="https://www.kaggle.com/c/histopathologic-cancer-detection/overview" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://www.kaggle.com/c/histopathologic-cancer-detection/overview"><strong class="markup--strong markup--mixtapeEmbed-strong">Histopathologic Cancer Detection</strong><br><em class="markup--em markup--mixtapeEmbed-em">Identify metastatic tissue in histopathologic scans of lymph node sections</em>www.kaggle.com</a><a href="https://www.kaggle.com/c/histopathologic-cancer-detection/overview" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="21160f312ae6dcb445fff7504eb9e526" data-thumbnail-img-id="0*lLxuvys_rA-gN4ua" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*lLxuvys_rA-gN4ua);"></a></div><div name="df4f" id="df4f" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://www.kaggle.com/aimdata/cancer-detection-using-cnn" data-href="https://www.kaggle.com/aimdata/cancer-detection-using-cnn" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://www.kaggle.com/aimdata/cancer-detection-using-cnn"><strong class="markup--strong markup--mixtapeEmbed-strong">Cancer Detection Using CNN</strong><br><em class="markup--em markup--mixtapeEmbed-em">Explore and run machine learning code with Kaggle Notebooks | Using data from Histopathologic Cancer Detection</em>www.kaggle.com</a><a href="https://www.kaggle.com/aimdata/cancer-detection-using-cnn" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="46e210d8a84326808d6fbd9343658f9d" data-thumbnail-img-id="0*nqdReUBBmSaMWAwD" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*nqdReUBBmSaMWAwD);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@precillieo" class="p-author h-card">Kolawole Precious</a> on <a href="https://medium.com/p/1acab6481d05"><time class="dt-published" datetime="2020-12-15T21:36:41.008Z">December 15, 2020</time></a>.</p><p><a href="https://medium.com/@precillieo/convolutional-neural-network-for-detecting-cancer-tumors-in-microscopic-images-1acab6481d05" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on December 24, 2020.</p></footer></article></body></html>